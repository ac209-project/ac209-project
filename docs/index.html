<!DOCTYPE html>
<html lang="en">

<head>

    <meta charset="utf-8">
    <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">
    <meta name="description" content="">
    <meta name="author" content="">
    <link rel="shortcut icon" href="favicon.ico" type="image/x-icon">

    <title>Spotify's Next Top Model</title>

    <!-- Bootstrap core CSS -->
    <link href="vendor/bootstrap/css/bootstrap.min.css" rel="stylesheet">

    <!-- Custom styles for this template -->
    <link href="css/simple-sidebar.css" rel="stylesheet">

    <!-- Our styles -->
    <link href="css/style.css" rel="stylesheet">

</head>

<body>

<div id="wrapper">

    <!-- Sidebar -->
    <div id="sidebar-wrapper">
        <ul class="sidebar-nav">
            <li class="sidebar-brand">
                <a href="#">
                    Project Menu
                </a>
            </li>
            <li>
                <a href="#probstatement">Problem Statement</a>
            </li>
            <li>
                <a href="#data">Introduction & Data</a>
            </li>
            <li>
                <a href="#lit">Literature Review</a>
            </li>
            <li>
                <a href="#models">Modeling & Trajectory</a>
            </li>
            <li>
                <a href="#results">Results</a>
            </li>
            <li>
                <a href="#references">References</a>
            </li>
            <li>
                <a href="#contact">Contact</a>
            </li>
        </ul>
    </div>
    <!-- /#sidebar-wrapper -->

    <!-- Page Content -->
    <div id="page-content-wrapper">
        <div class="container-fluid">

            <div class="row">
                <div class="col-md-12">
                    <h1>Spotify's Next Top Model</h1>
                    <p>
                        Welcome to our Harvard University, Fall 2017 - Data Science I final project page. Please use the
                        menu button below to jump to your section of choice.<br><br>
                        <a href="#menu-toggle" class="btn btn-secondary" id="menu-toggle">Toggle Menu</a>
                    </p>
                </div>
            </div>
            <hr>


            <div id="probstatement" class="row section">
                <div class="col col-md-12"><h2>Problem Statement & Motivation</h2></div>
            </div>

            <div class="row">
                <div class="col col-md-12">
                    <p>
                        The purpose of this project is to use publicly available data through Spotify's
                        <a href="https://developer.spotify.com/web-api/" target="_blank">API</a>, the
                        <a href="https://spotipy.readthedocs.io/en/latest/" target="_blank">Spotipy</a> python library,
                        <a href="https://github.com/ac209-project/ac209-project/tree/master/w2v" target="_blank">word-to-vector
                            feature engineering</a>, and <a href="https://www.usmagazine.com/celebrities/a/">celebrity
                        information</a> to
                        predict the success of Spotify user playlists, and create a tool to build successful playlists
                        based on user specified features (genre, artist, etc.).
                    </p>
                    <p>
                        Throughout this project "playlist success" (our response variable) is defined as the number of
                        playlist followers. We have employed machine-learning regression techniques taught throughout
                        the semester as well as
                        <a href="https://en.wikipedia.org/wiki/Word2vec" target="_blank">Word2vec</a> modeling to
                        predict playlist
                        success.
                    </p>
                </div>
            </div>
            <hr>

            <div id="data" class="row">
                <div class="col col-md-12">
                    <h2>Introduction & Description of Data</h2>
                    <p>In this section we will give you some insights in to the data we collected, aggregated, and
                        combined, as well as the in-depth exploratory data analysis (EDA) performed to try and
                        illustrate relationships surrounding playlist success - specifically artist categorization,
                        artist and playlist genres, and playlist popularity as it relates to time.</p>
                </div>
            </div>

            <div class="row">
                <div class="col col-md-12">
                    <p><b>Baseline Data:</b><br>
                        Baseline data was collected primarily from the Spotify API. After
                        sourcing the Spotify users who “own” (the playlists are attached to their accounts) a large
                        number of playlists, the spotipy library was employed to collect user playlists, tracks, user
                        followers, and genre data from the API, each saved as JSON files. Genre information for
                        each playlist was harder to source and incorporate than one would think. It does not come
                        directly from each playlist, but rather from the artists of each track as a separate query. We
                        also collected celebrity names from <a href="https://www.usmagazine.com/celebrities/a/">US
                            Magazine</a>.
                    <p>
                        Before combining into one comprehensive DataFrame, the separate files have the following basic
                        form:
                    </p>
                    <table class="table table-striped table-project">
                        <thead>
                        <tr>
                            <th>Data File</th>
                            <th>Observations</th>
                            <th>Features</th>
                        </tr>
                        </thead>
                        <tbody>
                        <tr>
                            <td>Usernames</td>
                            <td>846</td>
                            <td>1</td>
                        </tr>
                        <tr>
                            <td>Celebrity Names</td>
                            <td>840</td>
                            <td>1</td>
                        </tr>
                        <tr>
                            <td>Genres</td>
                            <td>1343</td>
                            <td>5</td>
                        </tr>
                        <tr>
                            <td>Playlists</td>
                            <td>8183</td>
                            <td>7</td>
                        </tr>
                        <tr>
                            <td>Tracks</td>
                            <td>367911</td>
                            <td>8</td>
                        </tr>
                        </tbody>
                    </table>
                    <p>
                        The files were combined and manipulated into a single DataFrame via the playlist ID, username,
                        and artist name attributes. While cleaning, we decided to eliminate any playlist with one or
                        fewer followers (the owner).
                    </p>
                    <p>
                        We explore the data (and our engineered features) at both the track and the playlist level. As
                        you can see above, we have about 9.7 playlists per user when we include all users. When we
                        drop playlists with below 1 follower, we have about 4.4 playlists per user. Likewise, we start
                        with ~368k tracks and after we drop the playlists with if they have one or fewer followers we
                        only have ~211k
                        tracks remaining.
                    </p>
                    <p>
                        This is clearly a biased subsample of playlists in that we sourced users that own high numbers
                        of playlists and removed those playlists with only one follower, but the flipside is that the
                        vast majority of Spotify users are only making playlists for themselves (one user), or maybe
                        sharing them with just friends or close loved ones. A cross-section of those users would make
                        our already right-skewed (as seen below) data even more skewed.</p>
                    <p>
                        Click <a href="https://github.com/ac209-project/ac209-project/tree/master/getplaylist"
                                 target="_blank">here</a>
                        to explore our data collection techniques.
                    </p>
                </div>
            </div>
            <hr>
            <div class="row">
                <div class="col col-md-8">
                    <p><b>Playlist Followers - A first look at playlist success:</b><br>
                        Engineering features around artist and playlist data yielded interesting relationships. A
                        cursory measure of playlist success, our generalized project goal, is the number of followers
                        per playlist. The distribution of playlist followers shows a significant right-skew, with most
                        playlists having few followers and a few approaching the 1M mark. The log-transform of followers
                        illustrates a potentially more useful response variable (right).
                    </p>
                </div>
                <div class="col col-md-4">
                    <img src="img/eda1.png" align="center" class="img">
                </div>
            </div>
            <hr>
            <div class="row">
                <div class="col-md-12">
                    <p><b>Categorizing artists:</b><br>
                        We further aggregated the artists by splitting them up into five distinct categories using
                        combinations of their mean popularity (as determined by Spotify's popularity metric) and total
                        number of artist tracks as seen in the following table.<br>

                        The motivation for the class boundary decisions was to keep the class numbers down to reasonably
                        small levels, but still explain the variance in artist performance. The 10-track boundary was
                        selected due to the shape of the data - it was designed to catch the slight nonlinearity in
                        artist success rates as they produce fewer songs. Then, the popularity boundaries were set to be
                        reasonably symmetrical around the normal curve while ensuring that each class had a significant
                        number of observations present.<br>

                        These categories should be
                        self-descriptive, but the two plots below do a fantastic job of illustrating each in terms of
                        their thresholds and relationships. The artist popularity metric ties in with our expectations
                        because we see people like Post Malone, Camila Cabello, and Ed Sheeran in our superstar
                        category.
                    </p>
                </div>
            </div>
            <div class="row">
                <div class="col-md-4">
                    <table class="table table-project">
                        <thead>
                        <tr>
                            <th>Artist Category</th>
                            <th>Artist Mean Track Popularity</th>
                            <th>Artist Total Tracks</th>
                            <th>Number of Tracks in Sample Produced by class</th>
                        </tr>
                        </thead>
                        <tbody class="bands">
                        <tr style="background-color: rgb(72,120,207)">
                            <td>Superstar</td>
                            <td>>= 50</td>
                            <td>>= 10</td>
                            <td>60k</td>
                        </tr>
                        <tr style="background-color: rgb(214,95,95)">
                            <td>Star</td>
                            <td>>= 20 and < 50</td>
                            <td>>= 10</td>
                            <td>62k</td>
                        </tr>
                        </tr>
                        <tr style="background-color: rgb(106,204,101)">
                            <td>One-hit-wonder</td>
                            <td>>= 40</td>
                            <td>< 10</td>
                            <td>27k</td>
                        </tr>
                        <tr style="background-color: rgb(180,124,199)">
                            <td>Garage Band</td>
                            <td>< 40</td>
                            <td>< 10</td>
                            <td>45k</td>
                        </tr>
                        <tr style="background-color: rgb(202,185,125)">
                            <td>Trash Factory</td>
                            <td>>= 0 and < 20</td>
                            <td>>= 10</td>
                            <td>14k</td>
                        </tbody>
                    </table>
                </div>
                <div class="col-md-8">
                    <img src="img/eda2.png" align="center" class="img"/>
                </div>
            </div>
            <hr>
            <div class="row">
                <div class="col-md-12">
                    <p><b>Continuing Artist Analysis:</b><br>
                        Below you can see a illustrative distribution of these five artist categories grouped by
                        playlist as they relate to number of playlist followers and specific track popularity (as
                        determined by Spotify). We see
                        that, unsurprisingly, not only do most playlists incorporate popular artists (superstar:blue
                        and star:red), but a
                        lot of them have the superstar artists’ best songs (the mostly blue right side), as well as a
                        smattering of one-hit-wonders (green in the middle) and a dense area of moderately popular songs
                        throughout (between 30 and 70 popularity). This relationship and playlist architecture should
                        be intuitive. You can also note, interestingly, that only one-hit-wonders (green) get perfect
                        100s for
                        track popularity.
                    </p>
                </div>
            </div>
            <div class="row">
                <div class='col-md-7 mx-auto'><img src="img/eda3.png" class="img"/></div>
            </div>
            <hr>
            <div class="row">
                <div class="col-md-12">
                    <p>
                        <b>Playlists & Time:</b><br>
                        It appears that there may be a relationship between the number of tracks a playlist has and its
                        number of followers (top-left), as well as length of its description (top-right). We also
                        investigated the impact of playlist turnover rate, or relative age, days between the oldest and
                        newest track being added (bottom-left), as well as the age of the oldest song on playlist
                        follower count (bottom-right).
                    </p>
                </div>
            </div>
            <div class="row">
                <div class='col-md-10 mx-auto'><img src="img/eda4.png" class="img"/></div>
            </div>
            <hr>
            <div class="row">
                <div class="col-md-12">
                    <p><b>Track Popularity:</b><br>
                        We also took a closer look at other playlist metrics, specifically average playlist track
                        popularity versus the number of tracks in a playlist or the maximum playlist track popularity as
                        seen in the following two plots, respectively. Does it matter more to have a few really popular
                        songs, or just a lot of decent songs? From our graphs below we would say having a few good songs
                        is more important. The graph on the right below shows the average playlist track popularity and
                        the max track popularity colored by playlist followers. We can see the playlists with lots of
                        followers (dark blue) are much more consistently in the section with high max track popularity
                        regardless of their mean track popularity.</p>
                </div>
            </div>
            <div class="row">
                <div class="col-md-10 mx-auto"><img src="img/eda5.png" class="img"/></div>
            </div>
            <hr>
            <div class="row">
                <div class="col-md-12">
                    <p>
                        <b>Genres:</b><br>
                        If you were to ask anyone on the street how they would measure the success of any playlist, one
                        of their first responses would probably be "Depends on the genre." This is a natural
                        categorization we do with music on a daily basis. It is typically how we choose what we are
                        going to listen to when we go to the gym, start our daily commute, or go for a hike in the
                        woods, so of course the predominant genres that make up a playlist will have some impact on the
                        relative success of that playlist against all others, as well as those geared toward the same
                        genre.
                    </p>
                    <p>
                        In this section you will see four distinct looks at how genres, at the artist, playlist,
                        and track levels, impact playlist success.<br>

                        <b>Artist Genres - </b>The top-left barplot shows the median number of playlist followers, so as
                        to be less sensitive
                        to outliers.

                        The top-right barplot is based on mean artist popularity by genre. As you can
                        see, there are some specific genres at the top that we would usually associate with one artist
                        (canadian pop - Justin Bieber, detroit hip hop - Eminem). It is likely those artists are skewing
                        the distribution, since those are their respective genres. Sadly, we do not have enough members
                        of each genre to get a good view of the most popular genres by artist.<br>

                        <b>Playlist Genres - </b>The bottom-left barplot look at the most common genre by number of
                        playlist occurrences. Here we use the "mode genre" field. This field is made by looking at all
                        the genres that make up a playlist (on a track level) and choosing the most common (mode).<br>

                        <b>Track Genres - </b>The bottom-right barplot takes a look at some "genre-type"
                        information. We are specifically looking at any sub-genres that fall in a larger, more
                        general genre. For example, 'classic rock', 'heavy rock', and 'swedish electronic pop rock'
                        would all fall in our 'rock' genre-type. For each genre related to a song we have binary
                        variables to indicate whether that genre-type is present in the track. The genres in this plot
                        represent the 15 most common genres in our dataset [alternative, Christmas, country, dance,
                        deep, hip hop, house, indie, jazz, latin, metal, pop, rap, rock, soul].<br>

                        <br><b>Note:</b> We decided to use the top 15 genres since this corresponds to industry
                        standards for categorizing music. For example, iTunes "search-by-genre" has twenty-two and
                        <i>Billboard 200</i> uses ten.
                    </p>
                </div>
            </div>
            <div class="row">
                <div class="col-md-6 mx-auto"><img src="img/eda7.png" class="img"/></div>
                <div class="col-md-6 mx-auto"><img src="img/eda8.png" class="img"/></div>
            </div>
            <div class="row">
                <div class="col-md-6 mx-auto"><img src="img/eda9.png" class="img"/></div>
                <div class="col-md-6 mx-auto"><img src="img/eda10.png" class="img"/></div>
            </div>
            <hr>
            <div class="row">
                <div class="col-md-8">
                    <p>
                        <b>Celebrity Playlist Owners:</b><br>
                        Lastly, we explored whether the owner of the playlist is an official spotify account. The
                        motivation for
                        this is that some of the most popular playlists we have seen have been from a celebrity's
                        official
                        Spotify account, and that generally celebrity accounts (such the Official Justin Bieber account)
                        tend to produce more followers. We would like to be able to account for this bias.</p>
                    <p>
                        As you can see, there are not a huge number of celebrities in our data, but their social status
                        inevitably has an impact on playlist popularity.
                    </p>
                </div>
                <div class="col-md-4 mx-auto"><img src="img/eda11.png" class="img"/></div>

            </div>
            <div class="row">
                <div class="col-md-12">
                    <p>Click <a href="https://github.com/ac209-project/ac209-project/tree/master/EDA" target="_blank">here</a>
                        for a closer look at our EDA techniques.</p>
                </div>
            </div>
            <hr>

            <div id="lit" class="row">
                <div class="col col-md-12"><h2>Literature Review & Related Work</h2></div>
            </div>

            <div class="row">
                <div class="col col-md-8">
                    <p>
                        A major contributing source for developing our 209-level feature using word-to-vec (w2v)
                        analysis of
                        playlist titles was <a
                            href="https://github.com/ac209-project/ac209-project/blob/master/w2v/word2vec_paper.pdf"
                            target="_blank">this paper</a> (Mikolov, Chen, Corrado, & Dean, 2013) entitled "Efficient
                        Estimation of Word Representations in Vector Space." The paper abstract follows:
                    </p>
                    <p style="text-align: justify; padding: 3%;"><i>We propose two novel model architectures for
                        computing continuous vector representations of words from very large data sets. The quality of
                        these representations is measured in a word similarity task, and the results are compared to the
                        previously best performing techniques based on different types of neural networks. We observe
                        large improvements in accuracy at much lower computational cost, i.e. it takes less than a day
                        to learn high quality word vectors from a 1.6 billion words data set. Furthermore, we show that
                        these vectors provide state-of-the-art performance on our test set for measuring syntactic and
                        semantic word similarities.</i>
                    </p>
                </div>
                <div class="col col-md-4">
                    <img src="img/w2v1.png" align="center" class="img"/>
                    <p class="caption">A preview of our w2v feature generation</p>
                </div>
            </div>
            <hr>


            <div id="models" class="row">
                <div class="col col-md-12"><h2>Modeling Approach & Project Trajectory</h2></div>
            </div>

            <div class="row">
                <div class="col col-md-12">
                    <p><b>Baseline Modeling:</b><br>
                        As we move into modeling and attempting to predict playlist success based on number of
                        followers, we again use the playlists (and related tracks) that have more than one follower. The
                        rational being that we do not want to use data that could skew the popularity metrics for tracks
                        if it is unlikely that those users have any followers that could potentially follow their
                        playlist. Essentially, we do not think those users have enough presence on Spotify for the
                        number of followers to be a meaningful feedback metric for playlist success. This left us with
                        a DataFrame consisting of 210,798 observations (tracks) across 49 measures, but since we are
                        predicting playlist followers and do not have time or experience to implement a full
                        hierarchical model, we aggregate our track, artist, and user information up to the playlist
                        level.
                    </p>
                    <p>
                        <b>Imputation:</b><br>
                        We also changed our datetime format columns to just include the year, and we median impute for
                        the (few) missing values in other columns. We used the median since there were few values to
                        impute and the distributions are skewed as seen in the first followers plot above, hence mean
                        imputation would be susceptible to outliers.
                    </p>
                    <p>
                        <b>One hot encoding and train/test split:</b><br>
                        Since we have a number of categorical measures, one hot encoding was applied to those features.
                        We also grouped tracks by playlist, reducing our observations from about 210,000 to a
                        DataFrame with ~3500 playlists across 480 features.
                    </p>
                    <p>
                        We split the data with 75% in the training set and 25% in test, standardized the
                        numerical columns, then took an iterative
                        approach to modeling, using 5-fold cross-validation throughout, and moved through eight
                        different regression techniques: ridge, lasso, elastic net, k-NN, random forest,
                        AdaBoost, XGBoost, and SVM.
                    </p>
                </div>
            </div>
            <hr>
            <div class="row">
                <div class="col col-md-12">
                    <p><b>Word2vec on Playlist Names:</b><br>
                        In order to go beyond the scope of 109A and incorporate a method not discussed in class, we
                        decided to apply a word-to-vec (w2v) transform to as many applicable words as there are in
                        the set of playlist names. The core intuition being that the playlist name is what represents
                        the playlist to the outside world. A bad playlist name can keep people from following or
                        listening to it, even if the content is great, while a good playlist name can draw users. The
                        playlist name also, hopefully, categorizes the playlist in a meaningful way we can use. This
                        categorization could be by genre, mood, activity or any other type of category the playlist
                        creator thought was useful.
                    </p>
                    <p>
                        Click <a href="https://github.com/ac209-project/ac209-project/tree/master/w2v" target="_blank">here</a>
                        to
                        explore the w2v model training and implementation specific to our project.
                    </p>
                    <p>
                        <b>W2V Technique:</b><br>
                        By using word-to-vec we seek to cluster the playlist names into meaningful categories based on
                        the normalized sum of the vectors of known words in the playlist. This will aggregate the vector
                        information from each word in the playlist description, with all unknown words set to the zero
                        vector. We normalize the vectors in one feature (we have a normalized and a not normalized
                        version of this feature) once we aggregate them to stop playlists with just many words from
                        being higher in magnitude than playlists with just a few words.
                    </p>
                    <p>
                        <b>W2V Difficulties:</b><br>
                        Ultimately, there will be some difficulties with misspellings, different languages, and internet
                        slang (or emojis), and we will doubtless lose information in this quick application of w2v, but
                        we think the output will still be a meaningful feature in our final model. We have corrected for
                        many of these as was quickly possible with some simple NLTK and regex tools.
                    </p>
                    <p>
                        <b>W2V Model:</b><br>
                        Our w2v model was trained with the Text8 wikipedia corpus. This corpus seemed to be the
                        most applicable one that was both a reasonable size to download to a personal computer and took
                        a reasonable amount of time to train. After training the model, the "sentences" formed
                        by the playlist names were added to the vocabulary of the model. Without proper
                        context the model will not denote a vector transform for a word, so there are still several
                        words (about 25%) that do not have a related vector representation.
                    </p>
                    <p>
                        About 84% of the playlist names have at least one meaningful word (not a stop word by NLTK
                        standards) that has a vector representation in our model. This number could doubtless be
                        improved with a music specific or more casual internet chat area like Twitter or Reddit dataset
                        as opposed to the more formal Wikipedia dataset, but there is only so much time to make our
                        features.
                    </p>
                    <p>
                        The below plot shows different word locations in a sample of our TSNE plot. There is some
                        structure to the word placement. Toward the bottom, there are several spanish
                        words and related genres (like salsa and classicos). We can also see some other consistencies,
                        like "kendrick" and "hip hop" are together in one group, and "party", "club" and "pill" are all
                        together in another group. The w2v transform is definitely noisy in this case though. We
                        think this is due to the smaller training set, and the makeup of the training set. The training
                        set is made up of wikipedia documents which, though they contain a lot of words, probably do not
                        have a lot of the same contexts or slang that we will see here. If we could train with a corpus
                        from Twitter or Reddit we might have had better context for the slang and pithy statements we
                        often
                        see in playlist names.
                    </p>
                    <div><img src="img/word_groupings.png" class="img"></div>
                </div>
            </div>
            <hr>
            <div class="row">
                <div class="col-md-6">
                    <p>
                        <b>Model Selection without w2v:</b><br>
                        We iterated over several regressor models (with each parameter set optimized by 5-fold
                        cross-validation) to determine which performs best with this data both without the w2v
                        feature included. Find the results to the right.
                    </p>
                    <p>
                        Note: We did not include Linear Regression for two reasons:
                    <ol>
                        <li>We cross validate with Ridge and Lasso over very small values for our regularization term.
                        </li>
                        <li>We looked at unregularized linear regression; it was bad and not worth the time to run it.
                        </li>
                    </ol>

                    <p>
                        We can see in these results that the best performing model on a cross-validated training set is
                        our random forest with 500 estimators.
                    </p>
                </div>
                <div class="col-md-6">
                    <table class="table table-striped table-project">
                        <thead>
                        <th>Model</th>
                        <th>Cross-validated validation<br>set R^2 mean & std<br>(5-folds)</th>
                        <th>Train Score</th>
                        <th>Test Score</th>
                        </thead>
                        <tbody>
                        <tr>
                            <td>Ridge Regression</td>
                            <td>-0.0003, 0.0591</td>
                            <td>0.0513</td>
                            <td>0.0353</td>
                        </tr>
                        <tr>
                            <td>Lasso Regression</td>
                            <td>-0.0241, 0.0441</td>
                            <td>0.0840</td>
                            <td>0.0412</td>
                        </tr>
                        <tr>
                            <td>Elastic Net Regression</td>
                            <td>-0.0028, 0.0625</td>
                            <td>0.0876</td>
                            <td>0.0409</td>
                        </tr>
                        <tr>
                            <td>Random Forest Regression</td>
                            <td>0.0522, 0.0424</td>
                            <td>0.8766</td>
                            <td>0.1441</td>
                        </tr>
                        </tbody>
                    </table>
                </div>
            </div>
            <hr>
            <div class="row">
                <div class="col-md-6">
                    <p>
                        <b>Model Selection with w2v:</b><br>
                        We iterated over several regressor models (with each parameter set optimized by 5-fold
                        cross-validation) to determine which performs best with this data both with the w2v
                        feature included. Find the results to the right.
                    </p>
                    <p>
                        We can see in these results that the best performing model on a cross-validated training set is
                        our random forest with 500 estimators.
                    </p>
                </div>
                <div class="col-md-6">
                    <table class="table table-striped table-project">
                        <thead>
                        <th>Model</th>
                        <th>Cross-validated validation<br>set R^2 mean & std<br>(5-folds)</th>
                        <th>Train Score</th>
                        <th>Test Score</th>
                        </thead>
                        <tbody>
                        <tr>
                            <td>Ridge Regression</td>
                            <td>-0.0001, 0.0606</td>
                            <td>0.0513</td>
                            <td>0.0353</td>
                        </tr>
                        <tr>
                            <td>Lasso Regression</td>
                            <td>-0.0302, 0.1113</td>
                            <td>0.0854</td>
                            <td>0.0414</td>
                        </tr>
                        <tr>
                            <td>Elastic Net Regression</td>
                            <td>-0.0028, 0.0625</td>
                            <td>0.0884</td>
                            <td>0.0410</td>
                        </tr>
                        <tr>
                            <td>k-NN Regression</td>
                            <td>0.0133, 0.0144</td>
                            <td>0.0923</td>
                            <td>0.0448</td>
                        </tr>
                        <tr>
                            <td>Random Forest Regression</td>
                            <td>0.0635, 0.0404</td>
                            <td>0.8786</td>
                            <td>0.1047</td>
                        </tr>
                        <tr>
                            <td>AdaBoost Regression</td>
                            <td>-0.0269, 0.0441</td>
                            <td>0.7277</td>
                            <td>0.0897</td>
                        </tr>
                        <tr>
                            <td>XGBoost Regression</td>
                            <td>-0.0631, 0.2525</td>
                            <td>0.4715</td>
                            <td>0.1106</td>
                        </tr>
                        <tr>
                            <td>SVM Regression</td>
                            <td>0.0502, 0.0144</td>
                            <td>0.1613</td>
                            <td>0.0632</td>
                        </tr>
                        </tbody>
                    </table>
                </div>
            </div>
            <hr>
            <div class="row">
                <div class="col-md-12">
                    <p>
                        <b>Feature Selection:</b><br>
                        Before moving on to the results and playlist generation, let's take a quick look at feature
                        selection.
                    </p>
                    <p>
                        We can see above that the best performing model on a cross-validated training set is our random
                        forest with 500 estimators. We will use that as our baseline model - but we note that some
                        over-fitting occurs on the training set. We believe this is due to the high number of features
                        (some of which are codependent). So next, we try to reduce the feature set overall and see if
                        this leads to an increase in model performance.
                    </p>
                    <p>
                        The 30 most common features below are generally consistent with our expectations - for example:
                    <ul>
                        <li>The followers of the playlist owner are highly predictive of playlist followers.</li>
                        <li>Having a playlist description makes followers more likely to follow.</li>
                        <li>One hit wonders and superstars are the most important class of bands in a playlist.</li>
                    </ul>
                    Next, we try fitting all of our models again on the first 50 important features to see if
                    the model performs better on the significantly reduced feature set. This was done algorithmically by
                    taking the models with the greatest feature_importance in our selected random forest classifier
                    using the inbuilt feature_importance parameter.
                    </p>
                    <p>
                        Note that certain features here do have elements of multicollinearity - for example, we have
                        created features user_f_mean, user_f_min, and user_f_max, which correspond to the average,
                        smallest, and largest subscriber numbers of all of the user's playlists. As a next step, we had
                        contemplated removing certain highly correlated features. However, as we can see below, model
                        performance on a cross-validated training set is generally worse
                        with a reduced feature set (vs. our full feature set). <br><br>So we will keep our baseline
                        model and our original random forest
                        regressor when we add in the w2v feature, without adjusting for multi-collinear features. Our
                        rationale here is that while min/max/mean features are partially correlated, each does add
                        unique predictive information. And empirical results show that our full set achieves superior
                        <i>R</i><sup>2</sup> values, so we feel comfortable proceeding with the random forest model on a
                        full feature set.
                    </p>
                </div>
            </div>
            <div class="row">
                <div class="col-md-6">
                    <img src="img/features.png" class="img"/>
                </div>
                <div class="col-md-6">
                    <table class="table table-striped table-project">
                        <thead>
                        <th>Model</th>
                        <th>Cross-validated validation<br>set R^2 mean & std<br>(5-folds)</th>
                        <th>Train Score</th>
                        <th>Test Score</th>
                        </thead>
                        <tbody>
                        <tr>
                            <td>Ridge Regression</td>
                            <td>-0.0013, 0.0603</td>
                            <td>0.0506</td>
                            <td>0.0350</td>
                        </tr>
                        <tr>
                            <td>Lasso Regression</td>
                            <td>-0.0312, 0.1104</td>
                            <td>0.0836</td>
                            <td>0.0404</td>
                        </tr>
                        <tr>
                            <td>Elastic Net Regression</td>
                            <td>-0.0029, 0.0626</td>
                            <td>0.0742</td>
                            <td>0.0420</td>
                        </tr>
                        <tr>
                            <td>k-NN Regression</td>
                            <td>0.0213, 0.0011</td>
                            <td>0.0959</td>
                            <td>0.0485</td>
                        </tr>
                        <tr>
                            <td>Random Forest Regression</td>
                            <td>-0.0257, 0.1805</td>
                            <td>0.8755</td>
                            <td>0.1292</td>
                        </tr>
                        <tr>
                            <td>AdaBoost Regression</td>
                            <td>0.0555, 0.0143</td>
                            <td>0.6542</td>
                            <td>0.0671</td>
                        </tr>
                        <tr>
                            <td>XGBoost Regression</td>
                            <td>-0.0661, 0.2469</td>
                            <td>0.4604</td>
                            <td>0.0988</td>
                        </tr>
                        <tr>
                            <td>SVM Regression</td>
                            <td>0.0445, 0.0323</td>
                            <td>0.0871</td>
                            <td>0.0502</td>
                        </tr>
                        </tbody>
                    </table>
                </div>
            </div>
            <hr>
            <div class="row">
                <div class="col-md-12">
                    <h4>Comparing models with and without the w2v feature:</h4>
                </div>
            </div>
            <div class="row">
                <div class="col-md-6">
                    <p><b>Modeling without the w2v feature:</b><br>
                    </p>
                    <table class="table table-striped table-project">
                        <thead>
                        <tr>
                            <th>Model</th>
                            <th>Train Score</th>
                            <th>Test Score</th>
                        </tr>
                        </thead>
                        <tbody>
                        <tr>
                            <td>Ridge Regression</td>
                            <td>0.0485</td>
                            <td>0.0335</td>
                        </tr>
                        <tr>
                            <td>Lasso Regression</td>
                            <td>0.0798</td>
                            <td>0.0390</td>
                        </tr>
                        <tr>
                            <td>Elastic Net Regression</td>
                            <td>0.0811</td>
                            <td>0.0390</td>
                        </tr>
                        </tbody>
                    </table>
                </div>
                <div class="col-md-6">
                    <p>
                        <b>Modeling with the w2v feature:</b><br>
                    </p>
                    <table class="table table-striped table-project">
                        <thead>
                        <th>Model</th>
                        <th>Train Score</th>
                        <th>Test Score</th>
                        </thead>
                        <tbody>
                        <tr>
                            <td>Ridge Regression</td>
                            <td>0.0488</td>
                            <td>0.0335</td>
                        </tr>
                        <tr>
                            <td>Lasso Regression</td>
                            <td>0.0813</td>
                            <td>0.0392</td>
                        </tr>
                        <tr>
                            <td>Elastic Net Regression</td>
                            <td>0.0729</td>
                            <td>0.0413</td>
                        </tr>
                        </tbody>
                    </table>
                </div>
            </div>
            <div class="row">
                <div class="col-md-12">
                    <p>
                        The additional clustering from our w2v feature does not add
                        much to the model as a whole. This is likely for two reasons:
                    <ol>
                        <li>The w2v model could just be too noisy to actually add value to the model.</li>
                        <li>The playlist name might not be that important in general when you have information about the
                            number of followers of the user who "owns" the playlist, or some other confounding variable.
                        </li>
                    </ol>
                    Essentially, any influence from the name of the playlist may be close to conditionally independent
                    of the response given one (or a few) of the other variables. For either of these two reasons, the
                    209-level w2v feature does not increase our model score. To test this hypothesis, we look at a quick
                    difference in the straightforward regression models and take the average of the improvement on
                    test accuracy with the feature vs without.
                    </p>
                    <p>
                        As seen to the right, the average improvement in score for each model with the w2v feature is
                        <b>2.147%</b>.
                    </p>

                </div>
            </div>
            <hr>
            <div class="row">
                <div class="col-md-12">
                    <h1>Spotify's "Playlist Success" Next Top Model is...(drumroll)</h1>
                    <h1 style="color:red;">
                        Random Forest without Word-to-vec!
                    </h1>
                    <p>
                        Please hold your applause until the end, and see the "Results, Conclusions, and Future Work
                        section below for analysis.
                    </p>
                </div>
            </div>
            <hr>
            <div class="row">
                <div class="col-md-6">
                    <p>
                        <b>Generating Playlists:</b><br>
                        Aside from modeling playlist success we also developed a model to generate a playlist of a
                        user-defined length and user-specified genre within the top 15 genres: alternative, christmas,
                        country, dance, deep, hip hop, metal, house, indie, jazz, latin, pop, rap, rock, and soul.
                    </p>
                    <p>
                        The model works by using our predictive model to predict a score for a playlist, and then
                        joining this with the unique track playlist combos. Then we aggregate up to individual track
                        id's, summing up the playlist scores. Once we are here, we have the data we need.
                    </p>
                    <p>
                        Now, when we run the generative model, we get just tracks that are related to the specified
                        genre, and treat that as a distribution to sample from. We then normalize the predicted "scores"
                        from our model and scale them so they are valid probabilities.
                    </p>
                    <p>
                        Finally we sample from the distribution of the songs in our genre with the
                        associated probabilities given to them by the model. A sample "Latin" 20-song playlist is found
                        to the right.
                    </p>
                    <p>
                        Note: Each song can possibly be found in multiple genres.
                    </p>
                </div>
                <div class="col-md-6">
                    <img src="img/playlist.png" class="img">
                    <div class="caption">Sample 20 song "Latin" playlist</div>
                </div>

            </div>
            <hr>


            <div id="results" class="row">
                <div class="col col-md-12"><h2>Results, Conclusions, & Future Work</h2></div>
            </div>
            <div class="row">
                <div class="col col-md-12">
                    <p><b>Results:</b><br>
                        The models we evaluated generally performed better on the test dataset when using the full set
                        of 480 predictors, so the following discussion is comparing the models resulting from fitting on
                        this full set of predictors.
                    </p>
                    <p>
                        The best performing model for predicting Spotify playlist success was random forest regression,
                        which achieved an <i>R</i><sup>2</sup> on the test set of 10.5%. We considered sqrt(<i>n</i>)
                        features when looking for the best split at each tree, where <i>n</i> was the total number of
                        predictors we used in fitting the model. Using cross-validation, we determined that the optimal
                        number of trees for the forest was 500. We were able to leave the maximum depth of the trees
                        unbounded without shouldering prohibitive computational costs. The second-best performing model
                        was the “Extreme Gradient Boosting” (XGBoost) algorithm with an <i>R</i><sup>2</sup> on the test
                        set of 11.1%. Using cross-validation to tune the hyperparameters, we set the model’s maximum
                        depth to 3 and number of estimators to 25. We chose the Random Forest over the XGBoost model
                        because it had a higher mean CV score. This implies the model in general was more consistent in
                        its performance on the validation set, and hopefully more generalizable.
                    </p>
                    <p>
                        <b>Conclusions:</b><br>
                        For a research domain as complex as an individual’s music preferences and the influence of the
                        broader music industry on track popularity, our results were strong. Through the
                        <i>R</i><sup>2</sup> results, we can conclude that our best performing model managed to account
                        for more than 10% of the variation in the number of playlist followers (about the mean). Random
                        Forest and XGBoost are foundational tools in the machine learning context, generally regarded as
                        relatively strong performers in problems where there are many predictors, of which many are
                        considered significant. In the context of the high level of complexity in this problem, it is
                        not surprising that these models performed the best.
                    </p>
                    <p>
                        With the word-to-vec (“w2v”) feature we used as our 209-level feature, we can conclude that our
                        hypothesis was somewhat true in that there is minor improvement in the predictive power of
                        relatively simpler models (Ridge, Lasso) when they incorporate the feature than when they don’t.
                        When incorporating our 209-level w2v feature we see the average test set score improve by 2.147%
                        in these models. We expected that a playlist’s name would be a decent proxy for the playlist’s
                        contents, and the results of incorporating our w2v feature shows signs this hypothesis could be
                        true. One hypothesis as to why there isn’t any meaningful improvement in the more complex models
                        (Random Forest, XGBoost) is that through their ensemble and bagging algorithms, all the
                        information contained in our word-to-vec feature is captured by some combination of the other
                        predictors. For example, the w2v representation may be strongly related to the distribution of
                        genres (i.e., the type of music in a playlist), which is what our hypothesis about the playlist
                        names was based on.
                    </p>
                    <p>
                        This is a difficult question to investigate, and it is one compounded by the weak
                        interpretability of Random Forest and XGBoost models. Yet, what we sacrificed in
                        interpretability, we compensated for with improved predictive power. Since the main goal of the
                        project was to build the most effective model at predicting playlist success rather than making
                        inferences on what are the primary drivers, we are fine to accept the added complexity and lower
                        interpretability of these models compared to the others we evaluated.
                    </p>
                    <p>
                        <b>Future Work:</b><br>
                        To improve the w2v feature and give it a more “fair” examination, we could look to train the w2v
                        transform on a corpus more reflective of the semantics in Spotify’s playlist names. Out of
                        convenience and computational constraints, we used the Wikipedia corpus. However, as noted in
                        the w2v section, only about 84% of the playlist names have at least one meaningful word (not a
                        stop word by NLTK standards) that has a vector representation in our model. To improve this and
                        consequently better explain the sample set of playlist names, we could look to see whether a
                        more music-specific or more casual internet chat area (e.g., Twitter or Reddit) dataset is
                        available with which to train the w2v transform.
                    </p>
                    <p>
                        The playlist generation model works great in notebook form. If given more time, and requisite
                        expertise, another feature of this project we would have liked to incorporate is a kind of
                        "webapp" where the reader could choose one of our predefined genres from a dropdown menu to
                        generate a playlist. Again as it stands, using the final project notebook works quite well.
                    </p>
                    <p>
                        Lastly, the model would likely benefit from incorporating other datasets pertaining to music,
                        artists and genres. Currently, our model is fit on predominantly Spotify-sourced data. One
                        challenge in this regard is that many other available datasets (such as the Million Song
                        Dataset) are skewed towards older (what some may consider more “classic”) tracks, whereas
                        Spotify’s playlists contain many recent tracks. In fact, Spotify prides itself on its ability to
                        serve as a source for aspiring artists to be heard. Therefore, we weren’t able to include
                        information from the Million Song Dataset because limiting our domain to songs where there was
                        overlap between Spotify and the Million Song Dataset would have significantly shrunk our
                        playlist domain.
                    </p>
                </div>
            </div>
            <hr>

            <div id="references" class="row">
                <div class="col col-md-12"><h2>References</h2></div>
            </div>
            <div class="row">
                <div class="col col-md-12">
                    <ul>
                        <li><a href="https://developer.spotify.com/web-api/" target="_blank">Spotify Web API</a></li>
                        <li><a href="https://spotipy.readthedocs.io/en/latest/" target="_blank">Spotipy</a> python
                            library.
                        </li>
                        <li>
                            "Efficient Estimation of Word Representations in Vector Space." <a
                                href="https://github.com/ac209-project/ac209-project/blob/master/w2v/word2vec_paper.pdf"
                                target="_blank">(Mikolov, Chen, Corrado, & Dean, 2013)</a>
                        </li>
                        <li>This project's <a href="https://github.com/ac209-project/ac209-project" target="_blank">GitHub
                            repository</a>.
                        </li>
                        <li>Course project guidelines <a href="https://github.com/cs109/a-2017/tree/master/Projects"
                                                         target="_blank">GitHub
                            repository</a></li>
                        <li>Website framework provided by <a
                                href="https://github.com/BlackrockDigital/startbootstrap-simple-sidebar"
                                target="_blank">BlackrockDigital</a>.
                        </li>
                    </ul>
                    </p>
                </div>
            </div>
            <hr>

            <div id="contact" class="row">
                <div class="col col-md-12"><h2>Contact</h2></div>
            </div>
            <div class="row">
                <div class="col col-md-9">
                    <p>This project was produced by the following students:</p>
                    <ul>
                        <li><a href="mailto:pblankley@g.harvard.edu">Paul Blankley</a></li>
                        <li><a href="mailto:ryanjanssen@g.harvard.edu">Ryan Janssen</a></li>
                        <li><a href="mailto:andrewlund@g.harvard.edu">Andrew Lund</a></li>
                        <li><a href="mailto:nathaniel_stein@g.harvard.edu">Nate Stein</a></li>
                    </ul>
                </div>
            </div>
        </div>
    </div>
</div>

<!-- Bootstrap core JavaScript -->
<script src="vendor/jquery/jquery.min.js"></script>
<script src="vendor/bootstrap/js/bootstrap.bundle.min.js"></script>

<!-- Menu Toggle Script -->
<script>
    $("#menu-toggle").click(function (e) {
//        e.preventDefault();
        $("#wrapper").toggleClass("toggled");
    });
</script>
</body>
</html>


<!--Table for possible use in results section-->
<!--<table class="table table-project">-->
<!--<thead>-->
<!--<tr>-->
<!--<th>Results without Word2vec</th>-->
<!--<th>Results with Word2vec</th>-->
<!--</tr>-->
<!--</thead>-->
<!--<tbody>-->
<!--<tr>-->
<!--<td>-->
<!--<table class="table table-striped">-->
<!--<thead>-->
<!--<th>Model</th>-->
<!--<th>Cross-validated validation<br>set R^2 mean & std<br>(5-folds)</th>-->
<!--<th>Train Score</th>-->
<!--<th>Test Score</th>-->
<!--</thead>-->
<!--<tbody>-->
<!--<tr>-->
<!--<td>Linear Regression</td>-->
<!--<td>-9.661837490455958e+20, 1.3658652789203555e+21</td>-->
<!--<td>0.132484290729</td>-->
<!--<td>-7.77393283572e+18</td>-->
<!--</tr>-->
<!--<tr>-->
<!--<td>Ridge Regression</td>-->
<!--<td>-0.030256576859918376, 0.11125313901459834</td>-->
<!--<td>0.0853779911556</td>-->
<!--<td>0.0413614492559</td>-->
<!--</tr>-->
<!--<tr>-->
<!--<td>Lasso Regression</td>-->
<!--<td>-0.030256576859918376, 0.11125313901459834</td>-->
<!--<td>0.0853779911556</td>-->
<!--<td>0.0413614492559</td>-->
<!--</tr>-->
<!--<tr>-->
<!--<td>Elastic Net Regression</td>-->
<!--<td>-0.0028011693799219937, 0.06249175213203975</td>-->
<!--<td>0.0884215423941</td>-->
<!--<td>0.0410473844718</td>-->
<!--</tr>-->
<!--<tr>-->
<!--<td>k-NN Regression</td>-->
<!--<td>0.013480236420100952, 0.014509798407915084</td>-->
<!--<td>0.0923044971552</td>-->
<!--<td>0.0447576215128</td>-->
<!--</tr>-->
<!--<tr>-->
<!--<td>Random Forest Regression</td>-->
<!--<td>0.06498151001454726, 0.03988285917502669</td>-->
<!--<td>0.878798558284</td>-->
<!--<td>0.102835631709</td>-->
<!--</tr>-->
<!--<tr>-->
<!--<td>AdaBoost Regression</td>-->
<!--<td>-0.06615510226947337, 0.14867997551870718</td>-->
<!--<td>0.471762846853</td>-->
<!--<td>0.0671178069851</td>-->
<!--</tr>-->
<!--<tr>-->
<!--<td>XGBoost Regression</td>-->
<!--<td>-0.06309052342991424, 0.25255449612691144</td>-->
<!--<td>0.471512394636</td>-->
<!--<td>0.110631159802</td>-->
<!--</tr>-->
<!--<tr>-->
<!--<td>SVM Regression</td>-->
<!--<td>0.050170049519681514, 0.014360689088760155</td>-->
<!--<td>0.161309635032</td>-->
<!--<td>0.0632341098191</td>-->
<!--</tr>-->
<!--</tbody>-->
<!--</table>-->
<!--</td>-->
<!--<td>-->
<!--<table class="table table-striped">-->
<!--<thead>-->
<!--<th>Model</th>-->
<!--<th>Cross-validated validation<br>set R^2 mean & std<br>(5-folds)</th>-->
<!--<th>Train Score</th>-->
<!--<th>Test Score</th>-->
<!--</thead>-->
<!--<tbody>-->
<!--<tr>-->
<!--<td>Linear Regression</td>-->
<!--<td>0</td>-->
<!--<td>0</td>-->
<!--<td>0</td>-->
<!--</tr>-->
<!--<tr>-->
<!--<td>Ridge Regression</td>-->
<!--<td>0</td>-->
<!--<td>0</td>-->
<!--<td>0</td>-->
<!--</tr>-->
<!--<tr>-->
<!--<td>Lasso Regression</td>-->
<!--<td>0</td>-->
<!--<td>0</td>-->
<!--<td>0</td>-->
<!--</tr>-->
<!--<tr>-->
<!--<td>Elastic Net Regression</td>-->
<!--<td>0</td>-->
<!--<td>0</td>-->
<!--<td>0</td>-->
<!--</tr>-->
<!--<tr>-->
<!--<td>k-NN Regression</td>-->
<!--<td>0</td>-->
<!--<td>0</td>-->
<!--<td>0</td>-->
<!--</tr>-->
<!--<tr>-->
<!--<td>Random Forest Regression</td>-->
<!--<td>0</td>-->
<!--<td>0</td>-->
<!--<td>0</td>-->
<!--</tr>-->
<!--<tr>-->
<!--<td>AdaBoost Regression</td>-->
<!--<td>0</td>-->
<!--<td>0</td>-->
<!--<td>0</td>-->
<!--</tr>-->
<!--<tr>-->
<!--<td>SVM Regression</td>-->
<!--<td>0</td>-->
<!--<td>0</td>-->
<!--<td>0</td>-->
<!--</tr>-->
<!--</tbody>-->
<!--</table>-->
<!--</td>-->
<!--</tr>-->
<!--</tbody>-->
<!--</table>-->

<!--Comparisson Table-->
<!--<table class="table table-striped table-project">-->
<!--<thead>-->
<!--<th>Model</th>-->
<!--<th>Train Score<br>with W2V</th>-->
<!--<th>Train Score<br>without W2V</th>-->
<!--<th>Test Score<br>with W2V</th>-->
<!--<th>Test Score<br>without W2V</th>-->
<!--</thead>-->
<!--<tbody>-->
<!--<tr>-->
<!--<td>Ridge Regression</td>-->
<!--<td>0.0813</td>-->
<!--<td>0.0798</td>-->
<!--<td>0.0392</td>-->
<!--<td>0.0390</td>-->
<!--</tr>-->
<!--<tr>-->
<!--<td>Lasso Regression</td>-->
<!--<td>0.0813</td>-->
<!--<td>0.0798</td>-->
<!--<td>0.0392</td>-->
<!--<td>0.0390</td>-->
<!--</tr>-->
<!--<tr>-->
<!--<td>Elastic Net Regression</td>-->
<!--<td>0.0729</td>-->
<!--<td>0.0811</td>-->
<!--<td>0.0413</td>-->
<!--<td>0.0390</td>-->
<!--</tr>-->
<!--</tbody>-->
<!--</table>-->
